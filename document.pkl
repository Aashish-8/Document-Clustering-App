{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc0bdcff-b597-409d-a260-3b8ffdabf5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:56:09.439 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.889 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Aashish Bhabal\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-27 18:56:09.892 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.894 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.897 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:56:09.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import docx  # python-docx for Word files\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Custom stopword list\n",
    "stopwords = set([\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "    'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "    'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', \n",
    "    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "    'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \n",
    "    'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', \n",
    "    'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'\n",
    "])\n",
    "\n",
    "def preprocess(text):\n",
    "    if not text.strip():  # Handle empty input\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text (split by spaces)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords from the tokens\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    # Ensure that the resulting string is non-empty after filtering\n",
    "    if not filtered_tokens:\n",
    "        return \"\"\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def read_file(file):\n",
    "    file_type = file.name.split('.')[-1].lower()  # Get the file extension\n",
    "    \n",
    "    if file_type == 'txt':\n",
    "        return file.read().decode('utf-8')  # Read text file\n",
    "    \n",
    "    elif file_type == 'pdf':\n",
    "        # Read PDF content using PyMuPDF\n",
    "        text = \"\"\n",
    "        with fitz.open(stream=file.read(), filetype=\"pdf\") as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    \n",
    "    elif file_type == 'docx':\n",
    "        # Read Word content using python-docx\n",
    "        doc = docx.Document(file)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        return \"\"  # If file is not supported\n",
    "\n",
    "# Streamlit interface\n",
    "st.title(\"Document Clustering App\")\n",
    "\n",
    "# File uploader (allowing multiple file uploads)\n",
    "uploaded_files = st.file_uploader(\"Upload Files\", type=[\"txt\", \"pdf\", \"docx\"], accept_multiple_files=True)\n",
    "\n",
    "if uploaded_files:\n",
    "    # Initialize lists to store documents and filenames\n",
    "    documents = []\n",
    "    filenames = []\n",
    "\n",
    "    for uploaded_file in uploaded_files:\n",
    "        content = read_file(uploaded_file)  # Read content based on file type\n",
    "        preprocessed_text = preprocess(content)  # Preprocess text\n",
    "        if preprocessed_text.strip():  # Check if the text is not empty\n",
    "            documents.append(preprocessed_text)\n",
    "            filenames.append(uploaded_file.name)\n",
    "\n",
    "    # Check if documents are empty after preprocessing\n",
    "    if not documents:\n",
    "        st.error(\"Error: All documents are empty after preprocessing!\")\n",
    "    else:\n",
    "        # Filter out empty or too short documents (less than 5 words after preprocessing)\n",
    "        documents = [doc for doc in documents if len(doc.split()) > 0]\n",
    "        filenames = [filename for idx, filename in enumerate(filenames) if len(documents[idx].split()) > 0]\n",
    "\n",
    "        if not documents:\n",
    "            st.error(\"Error: All documents are too short after preprocessing!\")\n",
    "        else:\n",
    "            # Proceed with TF-IDF and KMeans clustering if there are valid documents\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            X = vectorizer.fit_transform(documents)\n",
    "\n",
    "            # KMeans clustering\n",
    "            n_clusters = 3  # You can adjust this dynamically\n",
    "            model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            labels = model.fit_predict(X)\n",
    "\n",
    "            # Display results in a table\n",
    "            result_df = pd.DataFrame({'Filename': filenames, 'Cluster': labels})\n",
    "            st.write(result_df)\n",
    "\n",
    "            # Optional: Plotting using Plotly (2D PCA Plot)\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "            fig = px.scatter(\n",
    "                x=X_pca[:, 0], \n",
    "                y=X_pca[:, 1], \n",
    "                color=labels.astype(str),\n",
    "                hover_name=filenames,\n",
    "                title=\"ðŸ“Š Document Clusters (PCA 2D Plot)\"\n",
    "            )\n",
    "            st.plotly_chart(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618e6e91-148b-468f-adec-dbefce7bf9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'document.pkl'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"document.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbf3e0-af8b-4a6f-abf8-a197c3e97ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
